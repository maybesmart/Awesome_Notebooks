{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Outlier_preprocessing_explaination.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHGaNZGyOM4D",
        "colab_type": "text"
      },
      "source": [
        "**This notebook does not have proper codes and proper formatting.**\n",
        "\n",
        "I have written down points which are useful \n",
        "\n",
        "This Content is what I understood from the webminar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dWp8tkwACnG",
        "colab_type": "text"
      },
      "source": [
        "#**Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb7iKkA7IpFl",
        "colab_type": "text"
      },
      "source": [
        "## WHY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2fkuadkI1qU",
        "colab_type": "text"
      },
      "source": [
        "ML algorithms do not work well with numerical data on different scales.\n",
        "\n",
        "1) scaling --> within specific min and max ranges --> (shifted and rescaled)\n",
        "\n",
        "2) standaradization --> centre your data around the mean.\n",
        "thus it will subtract every value of the numerical feature and divide each value by variance/std deviation\n",
        "thus features have 0 mean and unit variance\n",
        "\n",
        "they are expressed in \"z-score\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x29oelLwA1C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Normalization: transform a feature vector to unit norm(magnitude)\n",
        "magnitude of vector = 1\n",
        "Normalization definition: process of scaling input vectors individually to unit norm, often in order to simplify cosine similarity calculations\n",
        "\n",
        "\n",
        "Cosine similarity: measure similarity btw 2 non zero meters \n",
        "Note: L2 normalizaton simplifies cosine similarity calculation\n",
        "\n",
        "\n",
        "Note: normalizing --> row wise operation\n",
        "scaling --> column wise operation\n",
        "\n",
        "\n",
        "Normalization types:\n",
        "\n",
        "L1: sum of abs values of components of vector = 1\n",
        "L2: traditional def of vector magnitde\n",
        "max: largest abs value of the vector = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHDtNyQKkMd",
        "colab_type": "text"
      },
      "source": [
        "Transformers:\n",
        "\n",
        "**power transformer**: maps features in any distribution to Gaussian distribution --> used when we need 0 mean 1 unit variance normally distributed\n",
        "\n",
        "a) BOX-COX --> requires strictly positive input data\n",
        "\n",
        "b) yeo-johnson --> +,- input\n",
        "\n",
        "can be used on chi-squared distribution to get normal distribution\n",
        "\n",
        "\n",
        "**Quantile Transformer:** transform features to follow normal using quantile info. \n",
        "\n",
        "this non-linear can distort the correlation and linear relationships\n",
        "\n",
        "used for uniform, bimodal --> transform to gaussian normal distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYD1CzlVC6kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import RobustScaler, robust_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm1n02Q9GV1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# u can scale the data using scale(df)\n",
        "# then make a boxplot with 2 or 3 variables\n",
        "# boxplot itself will show outliers\n",
        "\n",
        "\n",
        "# in boxplot --> line is median\n",
        "# boundary for box is 25% and 75% (interquartile range)\n",
        "# the last ends are 1.5 times of 25% and 1.5 times of 75% (for both ends)\n",
        "# use standard scaler\n",
        "# repeat the process\n",
        "\n",
        "\n",
        "# use RobustScaler if u have lot of outliers\n",
        "# subtract median from every datapoint and divide by interquartile range\n",
        "\n",
        "\n",
        "# robust_scaled_df = pd.DataFrame(robust_scale(df), columns = df.columns)\n",
        "# then use .describe()\n",
        "# medians -->0\n",
        "# note: it does not get rid of outliers\n",
        "robust_scaler = RobustScaler(copy=True, with_centering=True, with_scaling=True)\n",
        "robust_scaler.fit_transform(df)\n",
        "result will be a np array\n",
        "# then represent using boxplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-T4DXRLIMQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Normalizing usig L1, L2, max norm\n",
        "\n",
        "l2_normalized_df = pd.DataFrame(normalize(df, norm='l2'), columns = df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcDdb2D1Jj7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_similarity(df.iloc[10].values.reshape(-1,1), df.iloc[11].values.reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTU6VDZJJxpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "at index 10 and 11,  high value is close to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dHE4-rMahG",
        "colab_type": "text"
      },
      "source": [
        "# Transform data to a normal distribution using quantile transformer:\n",
        "\n",
        "non-linear in nature\n",
        "\n",
        "advantage :  renders variable measured in diff scales to be comparable side by side\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "\n",
        "Used when \"Bimodally distributed \" --> when there are two sets of visible clusters --> eg)there is big gap btw the points in a single variable (at (1,2,3,4) then at (9,10,11))\n",
        "\n",
        "transformer = QuantileTransformer(output_distribution = 'normal', n_quantile = 200)\n",
        "\n",
        "store_transform = transformer.fit_transform(df[['a', 'b']])\n",
        "\n",
        "### it transform each variable\n",
        "### spreads out most frequently used values and reduces impact of outliers\n",
        "### very useful in cases of \"store visit\" , \"renewable hours (sun)\"\n",
        "### USE WHEN U CANNOT USE A CATEGORICAL VARIABLE (store id) TO HELP WITH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Hx6skjL6z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# very high dimensionality --> \n",
        "from sklearn.decomposition import FactorAnalysis # using SVD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8IGi8BmOpj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outlier: differ significantly from other data points in the same dataset (train)\n",
        "\n",
        "novelty: data point encountered in prediction and differ significantly from other data points in the test dataset\n",
        "\n",
        "outlier algo --> unsupervised --> never form a dense cluster --> sparse\n",
        "\n",
        "novelity --> semi supervised --> forms a dense cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzQXg3QLPTXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "*identify outlier --> 1)distance from mean,  2) distance from fitted line\n",
        "\n",
        "*coping with outliers --> drop, cap/floor, set to mean etc.\n",
        "\n",
        "\n",
        "1) dist from mean --> points that lie more than 3 standard deviationan from mean are considerd as outliers\n",
        "\n",
        "2) dist from fitted line --> points too far away from the fitted line\n",
        "\n",
        "if errorenous observation --> drop outliers\n",
        "\n",
        "set to mean if only few outlier\n",
        "\n",
        "if genuine/legitimate leave it\n",
        "\n",
        "set min max threshold to prevent it from affecting model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngfHH_k9NJP5",
        "colab_type": "text"
      },
      "source": [
        "# the below algo works for novelty and outlier detction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_OzjQSlQtRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Local outlier factor ---> uses nearest neighbors algo \n",
        "\n",
        "elliptic envelope --> tries to fit an eliptic envelope on the central mode, points outside are outliers\n",
        "\n",
        "isolation forest --> uses random forest of DT to identify outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWO-x0r8XH-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOcal outlier factors: for every point computed LOF score.\n",
        "if point is far from its nearest neighbor it is flagged as an outlier\n",
        "If those neighbors are close to each other\n",
        "\n",
        "number of neighbors to be considered into consideration can be set as a hyperparameter\n",
        "how close the neighbors are to each other (avg density) for each point\n",
        "\n",
        "if avg density is greater for the candidate pooint then it is a outlier.\n",
        "\n",
        "works well with moderate to high dimsension data\n",
        "\n",
        "NOTE: you must set explicitely in hyper params that novelty = True\n",
        "\n",
        "sklaern.neighbors.LocallyOutlierFatxore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjOm6ApYVER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elliptic envelope: assumes data is drawn from a normal (gaussian distribution)\n",
        "\n",
        "draws ellipse through central mode.\n",
        "\n",
        "\"robust covariance \"\n",
        "\n",
        "covariance represent relationship btw 2 variables\n",
        "it summarises pair wise covariance of vectors\n",
        "\n",
        "if greater/smaller vary correspond to greater or smaller var in the other var too\n",
        "\n",
        "covariance --> measured using maximum likelihood estimator, but this is sensitive to outlier\n",
        "\n",
        "the alternative way to meansure covariance is:\n",
        "\n",
        "***minimum covariance determinant --> uses complex but robust procedure\n",
        "this uses Mahalanobis distance.\n",
        "it is robust to outliers\n",
        "\n",
        "mahalanobis distacne: mesure of dist btw 2 points, similar to L2(euclidian) distance, but with the differnece that; each dimension is normalized to have equal variance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXXWT3ojZ9bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Isolation forest: uses RF\n",
        "for every data point , IF selects a feture for the datapoint. it then splits records based on randomly chosen value of the feature.\n",
        "this continues till a sample is isolated\n",
        "\n",
        "it will find by itself how many splits are needed to isolate a point,\n",
        "place the point in a category by itself. --> place the sample in a LEAF node, where in that leaf node only that point is available\n",
        "\n",
        "you can calculate how far that LEAF node is from the root of the tree.\n",
        "thus smaller the number of splits --> outlier\n",
        "points that have smaller path length from root --> outlier\n",
        "\n",
        "it averages path length from root to leafnode. if shorter --> outlier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdNpEB_TbdnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#dataset not available\n",
        "#but the variables are in this format\n",
        "\n",
        "student | score | no of hours \n",
        "\n",
        "# In the dataset, information about student, their score in a test and the number of hours they studied are used to preict the Outliers.\n",
        "#They determine whether the student is extremly intelligent or A student is hardworking but does not perform well in exams (fear)\n",
        "\n",
        "\n",
        "#our aim is to find outliers (after visualizing in scatter plot we observed a S curve, and the outliers where students studied less hrs, got more marks and vice versa)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COuA0dJOfRP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unsupervised\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors = 20, contamination = 0.2) # uses density\n",
        "# contamination refers to proportion of outliers that exist in the dataset.\n",
        "\n",
        "y_pred = lof.fit_predict(x)\n",
        "\n",
        "-1 -> outliers\n",
        "1 -> normal\n",
        "\n",
        "n_outliers = (y_pred == -1).sum()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UPF1d6Yf_6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to visualize the outliers,\n",
        "\n",
        "colors = np.array(['r','b'])\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "plt.title('LOF', size = 18)\n",
        "\n",
        "plt.scsatter(df['hours'], df['score'], s=100, color = colors[(y_pred + 1) // 2])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FX1FkqugiVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_score = lof.negative_outlier_factor --> if close to -1 --> inliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi8LKCuNhAWQ",
        "colab_type": "text"
      },
      "source": [
        "# isolation forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xomiPuhgxat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isf = IsolationForest(behaviour = 'new', contamination = 0.19) # proportion of outliers, behaviour=new uses threshold of zero\n",
        "\n",
        "y_pred = isf.fit_predict(x)\n",
        "\n",
        "-1 --> outliers\n",
        "1 --> normal\n",
        "\n",
        "# plot nd see using above method"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtjodM9Qhxbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "# all data comes from gaussian\n",
        "\n",
        "ee = EllipticEnvelope(support_fraction = 1., contamination = 0.19) # prop of data points we want to be included in MCD estimate.\n",
        "\n",
        "y_pred = ee.fit_predict(x)\n",
        "\n",
        "y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq1zN_HVjBRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx, yy = np.meshgrid(np.linspace(min(1st var) or 0, max(1st var) or max + 10, 500)), np.linspace(min(2ndt var) or 0, max(2nd var) or max + 10, 500)\n",
        "\n",
        "print('xx=', xx.ravel()) # converts to a 1d array\n",
        "print('yy =', yy.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SH5i189kvbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.c_[xx.ravel(), yy.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPJogomslCuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = isf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "z = z.reshape(xx.shape)\n",
        "\n",
        "z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G5GSBjDlMU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to visualize the outliers,\n",
        "\n",
        "colors = np.array(['r','b'])\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "plt.title('isf', size = 18)\n",
        "\n",
        "plt.scsatter(df['hours'], df['score'], s=100, color = colors[(y_pred + 1) // 2])\n",
        "\n",
        "plt.contour(xx ,yy,z, levels=[0], linewidths=2, colors='black')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGcvSvfulyeS",
        "colab_type": "text"
      },
      "source": [
        "ee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqKTTMEslqv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx, yy = np.meshgrid(np.linspace(min(1st var) or 0, max(1st var) or max + 10, 500)), np.linspace(min(2ndt var) or 0, max(2nd var) or max + 10, 500)\n",
        "\n",
        "print('xx=', xx.ravel()) # converts to a 1d array\n",
        "print('yy =', yy.ravel())\n",
        "\n",
        "z = ee.predict((np.c_[xx.ravel(), yy.ravel()])\n",
        "z = z.reshape(xx.shape)\n",
        "\n",
        "z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW-1Vszpl_DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to visualize the outliers,\n",
        "\n",
        "colorss = np.array(['r','b'])\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "plt.title('EE', size = 18)\n",
        "\n",
        "plt.scatter(df['hours'], df['score'], s=100, color = colorss[(y_pred + 1) // 2])\n",
        "\n",
        "plt.contour(xx ,yy,z, levels=[0], linewidths=2, colors='black')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hF9BbR8OBMl",
        "colab_type": "text"
      },
      "source": [
        "# Novelty Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7HndtA1sFVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Novelty detection\n",
        "\n",
        "lof_novelty = LocalisedOutlierFactor(n_neighbors =5, contamination = 0.19, novelty= True)\n",
        "\n",
        "lof_novelty.fit(x_train) # only fit\n",
        "\n",
        "y_pred = lof.predict(x_test)\n",
        "\n",
        "inlier = np.array([[500,00]])\n",
        "outlier = np.array([[10000, 40000]])\n",
        "\n",
        "lof_novelty.predict(inlier), lof_novelty.predict(outlier)\n",
        "\n",
        "lof_novelty.score_samples(inliers)  --> +ve score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "isf.predict(inlier), idf.predict(outlier)\n",
        "isf.score_samples(inlier), isf.score_sample(outlier)\n",
        "# measure of normality\n",
        "\n",
        "\n",
        "isf.descion_function(inlier)\n",
        "\n",
        "similarly\n",
        "ee.score_samples(inliers)  --> -ve of mahalobis dist\n",
        "NOTE: ee.mahalanobis\n",
        "\n",
        "ee.decision_function(inlier)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}